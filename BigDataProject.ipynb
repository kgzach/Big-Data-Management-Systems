{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0pp2p9NEH9J"
      },
      "source": [
        "# Εργαστηριακή Άσκηση Συστημάτων Διαχείρησης Μεγάλου Όγκου Δεδομένων"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msYcdeFt5b5I",
        "outputId": "f1a4c5a2-5f20-488c-9285-1d07a5c74784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.10/dist-packages (4.7.2)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo) (2.6.1)\n",
            "Requirement already satisfied: kafka-python in /usr/local/lib/python3.10/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: uxsim in /usr/local/lib/python3.10/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.21.5 in /usr/local/lib/python3.10/dist-packages (from uxsim) (1.25.2)\n",
            "Requirement already satisfied: matplotlib>=3.5.2 in /usr/local/lib/python3.10/dist-packages (from uxsim) (3.7.1)\n",
            "Requirement already satisfied: pillow>=9.2.0 in /usr/local/lib/python3.10/dist-packages (from uxsim) (9.4.0)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from uxsim) (4.66.4)\n",
            "Requirement already satisfied: scipy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from uxsim) (1.11.4)\n",
            "Requirement already satisfied: pandas>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from uxsim) (2.0.3)\n",
            "Requirement already satisfied: PyQt5>=5.15.7 in /usr/local/lib/python3.10/dist-packages (from uxsim) (5.15.10)\n",
            "Requirement already satisfied: dill>=0.3.8 in /usr/local/lib/python3.10/dist-packages (from uxsim) (0.3.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.2->uxsim) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.2->uxsim) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.2->uxsim) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.2->uxsim) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.2->uxsim) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.2->uxsim) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.2->uxsim) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.4->uxsim) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.4->uxsim) (2024.1)\n",
            "Requirement already satisfied: PyQt5-sip<13,>=12.13 in /usr/local/lib/python3.10/dist-packages (from PyQt5>=5.15.7->uxsim) (12.13.0)\n",
            "Requirement already satisfied: PyQt5-Qt5>=5.15.2 in /usr/local/lib/python3.10/dist-packages (from PyQt5>=5.15.7->uxsim) (5.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.5.2->uxsim) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pymongo\n",
        "!pip install kafka-python\n",
        "!pip install pyspark\n",
        "!pip install uxsim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "kiqk5na6_9dk"
      },
      "outputs": [],
      "source": [
        "import pymongo\n",
        "import kafka\n",
        "import pyspark\n",
        "import uxsim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "gF6IPIhtDdHz",
        "outputId": "be6b246d-b3b9-4851-db91-657d757782df"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"from google.colab import userdata\\nkafka_broker_scrt = userdata.get('kafka_broker_scrt')\\nmongo_conn = userdata.get('mongo_connect')\""
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''from google.colab import userdata\n",
        "kafka_broker_scrt = userdata.get('kafka_broker_scrt')\n",
        "mongo_conn = userdata.get('mongo_connect')'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "p4JyaAaCBwGJ"
      },
      "outputs": [],
      "source": [
        "# Link for Mongo Atlas Connection\n",
        "# https://cloud.mongodb.com/v2/663261fce5394b6dc6ded3ab#/metrics/replicaSet/6632625ff8721864dff8250d/explorer/BigData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWJ9Mrub4I_2",
        "outputId": "f3aea7cf-31eb-4d5c-e1d8-9a8f23769cd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<kafka.producer.future.FutureRecordMetadata at 0x7976bbbc9750>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from kafka import KafkaProducer\n",
        "# SDMD-1072573_78\n",
        "kafka_broker = '150.140.142.67:9094'\n",
        "#kafka_broker = kafka_broker_scrt\n",
        "producer = KafkaProducer(bootstrap_servers=kafka_broker)\n",
        "\n",
        "# Produce a message to a Kafka topic\n",
        "producer.send('Test', b'Hello, Kafka!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0mao0Ovl47QY",
        "outputId": "fd665678-3952-4871-d6fc-96eb86138c06"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"from kafka import KafkaConsumer\\n\\nconsumer = KafkaConsumer('SDMD-1072573_78', bootstrap_servers=kafka_broker)\\n\\n# Consume messages from the Kafka topic\\nfor message in consumer:\\n    print(message.value.decode('utf-8'))\""
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''from kafka import KafkaConsumer\n",
        "\n",
        "consumer = KafkaConsumer('SDMD-1072573_78', bootstrap_servers=kafka_broker)\n",
        "\n",
        "# Consume messages from the Kafka topic\n",
        "for message in consumer:\n",
        "    print(message.value.decode('utf-8'))'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwvaFNv95glb",
        "outputId": "40893402-767b-43c4-8cc5-dd14d8539590"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "collection Test already exists\n",
            "fail\n"
          ]
        }
      ],
      "source": [
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "uri = 'mongodb+srv://up1072573_78:BigBata73_78.10@bigdata.u9yjh5u.mongodb.net/'\n",
        "#uri = mongo_conn\n",
        "# Create a new client and connect to the server\n",
        "client = MongoClient(uri, server_api=ServerApi('1'))\n",
        "#client = MongoClient(uri)\n",
        "# Send a ping to confirm a successful connection\n",
        "try:\n",
        "    #client.admin.command('ping')\n",
        "    db = client['BigData']\n",
        "    db.create_collection('Test')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    print('fail')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXGfo5rSvURB"
      },
      "source": [
        "## Ερωτημα 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Gd2GegZhvTjB",
        "outputId": "9f6b08c9-a9d8-4e74-a176-fab794172568"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "simulation setting:\n",
            " scenario name: \n",
            " simulation duration:\t 3600 s\n",
            " number of vehicles:\t 14770 veh\n",
            " total road length:\t 6500 m\n",
            " time discret. width:\t 5 s\n",
            " platoon size:\t\t 5 veh\n",
            " number of timesteps:\t 720\n",
            " number of platoons:\t 2954\n",
            " number of links:\t 13\n",
            " number of nodes:\t 14\n",
            " setup time:\t\t 1.86 s\n",
            "simulating...\n",
            "      time| # of vehicles| ave speed| computation time\n",
            "       0 s|        0 vehs|   0.0 m/s|     0.00 s\n",
            "     600 s|      565 vehs|   4.5 m/s|     1.24 s\n",
            "    1200 s|      575 vehs|   4.7 m/s|     3.77 s\n",
            "    1800 s|      570 vehs|   4.5 m/s|     6.38 s\n",
            "    2400 s|      575 vehs|   4.3 m/s|     8.49 s\n",
            "    3000 s|      570 vehs|   4.3 m/s|    10.34 s\n",
            "    3595 s|      585 vehs|   2.9 m/s|    11.97 s\n",
            " simulation finished\n",
            "results:\n",
            " average speed:\t 7.2 m/s\n",
            " number of completed trips:\t 6945 / 14770\n",
            " average travel time of trips:\t 861.1 s\n",
            " average delay of trips:\t 820.7 s\n",
            " delay ratio:\t\t\t 0.953\n"
          ]
        }
      ],
      "source": [
        "#https://toruseo.jp/UXsim/docs/index.html\n",
        "#      Example code that was given\n",
        "from uxsim import *\n",
        "import itertools\n",
        "\n",
        "seed = None\n",
        "\n",
        "W = World(\n",
        "    name=\"\",\n",
        "    deltan=5,\n",
        "    tmax=3600, #1 hour simulation\n",
        "    print_mode=1, save_mode=0, show_mode=1,\n",
        "    random_seed=seed,\n",
        "    duo_update_time=600\n",
        ")\n",
        "random.seed(seed)\n",
        "\n",
        "# network definition\n",
        "\"\"\"\n",
        "    N1  N2  N3  N4\n",
        "    |   |   |   |\n",
        "W1--I1--I2--I3--I4-<E1\n",
        "    |   |   |   |\n",
        "    v   ^   v   ^\n",
        "    S1  S2  S3  S4\n",
        "\"\"\"\n",
        "\n",
        "signal_time = 20\n",
        "sf_1=1\n",
        "sf_2=1\n",
        "\n",
        "I1 = W.addNode(\"I1\", 1, 0, signal=[signal_time*sf_1,signal_time*sf_2])\n",
        "I2 = W.addNode(\"I2\", 2, 0, signal=[signal_time*sf_1,signal_time*sf_2])\n",
        "I3 = W.addNode(\"I3\", 3, 0, signal=[signal_time*sf_1,signal_time*sf_2])\n",
        "I4 = W.addNode(\"I4\", 4, 0, signal=[signal_time*sf_1,signal_time*sf_2])\n",
        "W1 = W.addNode(\"W1\", 0, 0)\n",
        "E1 = W.addNode(\"E1\", 5, 0)\n",
        "N1 = W.addNode(\"N1\", 1, 1)\n",
        "N2 = W.addNode(\"N2\", 2, 1)\n",
        "N3 = W.addNode(\"N3\", 3, 1)\n",
        "N4 = W.addNode(\"N4\", 4, 1)\n",
        "S1 = W.addNode(\"S1\", 1, -1)\n",
        "S2 = W.addNode(\"S2\", 2, -1)\n",
        "S3 = W.addNode(\"S3\", 3, -1)\n",
        "S4 = W.addNode(\"S4\", 4, -1)\n",
        "\n",
        "#E <-> W direction: signal group 0\n",
        "for n1,n2 in [[W1, I1], [I1, I2], [I2, I3], [I3, I4], [I4, E1]]:\n",
        "    W.addLink(n2.name+n1.name, n2, n1, length=500, free_flow_speed=50, jam_density=0.2, number_of_lanes=3, signal_group=0)\n",
        "\n",
        "#N -> S direction: signal group 1\n",
        "for n1,n2 in [[N1, I1], [I1, S1], [N3, I3], [I3, S3]]:\n",
        "    W.addLink(n1.name+n2.name, n1, n2, length=500, free_flow_speed=30, jam_density=0.2, signal_group=1)\n",
        "\n",
        "#S -> N direction: signal group 2\n",
        "for n1,n2 in [[N2, I2], [I2, S2], [N4, I4], [I4, S4]]:\n",
        "    W.addLink(n2.name+n1.name, n2, n1, length=500, free_flow_speed=30, jam_density=0.2, signal_group=1)\n",
        "\n",
        "\n",
        "# random demand definition every 30 seconds\n",
        "dt = 30\n",
        "demand = 2 #average demand for the simulation time\n",
        "demands = []\n",
        "for t in range(0, 3600, dt):\n",
        "    dem = random.uniform(0, demand)\n",
        "    for n1, n2 in [[N1, S1], [S2, N2], [N3, S3], [S4, N4]]:\n",
        "        W.adddemand(n1, n2, t, t+dt, dem*0.25)\n",
        "        demands.append({\"start\":n1.name, \"dest\":n2.name, \"times\":{\"start\":t,\"end\":t+dt}, \"demand\":dem})\n",
        "    for n1, n2 in [[E1, W1], [N1, W1], [S2, W1], [N3, W1],[S4, W1]]:\n",
        "        W.adddemand(n1, n2, t, t+dt, dem*0.75)\n",
        "        demands.append({\"start\":n1.name, \"dest\":n2.name, \"times\":{\"start\":t,\"end\":t+dt}, \"demand\":dem})\n",
        "\n",
        "W.exec_simulation()\n",
        "W.analyzer.print_simple_stats()\n",
        "W.analyzer.basic_to_pandas()\n",
        "W.analyzer.link_to_pandas()\n",
        "#W.analyzer.link_traffic_state_to_pandas().head(20)\n",
        "W.analyzer.od_to_pandas()\n",
        "#W.analyzer.vehicles_to_pandas().head(20)\n",
        "#W.analyzer.plot_vehicle_log(\"0\")\n",
        "df = W.analyzer.link_to_pandas()\n",
        "df['av_demand'] = (df['traffic_volume']+df['vehicles_remain'])/3600\n",
        "df['signal_time'] = signal_time\n",
        "#df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m8Z66j48-R-"
      },
      "source": [
        "#### Ερώτημα 1.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_8NylozF5H6",
        "outputId": "a4af1d6d-3ca8-4fb3-b846-4f87a96d0595"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic 'SDMD-1072573_78_vehicle_positions' created successfully\n"
          ]
        }
      ],
      "source": [
        "from kafka.admin import KafkaAdminClient, NewTopic\n",
        "\n",
        "def create_kafka_topic(bootstrap_servers, topic_name, num_partitions, replication_factor):\n",
        "    admin_client = KafkaAdminClient(\n",
        "        bootstrap_servers=bootstrap_servers,\n",
        "        client_id='1072573_78'\n",
        "    )\n",
        "\n",
        "    topic_list = [NewTopic(name=topic_name, num_partitions=num_partitions, replication_factor=replication_factor)]\n",
        "\n",
        "    try:\n",
        "        admin_client.create_topics(new_topics=topic_list, validate_only=False)\n",
        "        print(f\"Topic '{topic_name}' created successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to create topic '{topic_name}': {e}\")\n",
        "    finally:\n",
        "        admin_client.close()\n",
        "\n",
        "create_kafka_topic(bootstrap_servers=kafka_broker, topic_name='SDMD-1072573_78_vehicle_positions', num_partitions=3, replication_factor=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7EZeur59E2M"
      },
      "source": [
        "#### Ερώτημα 1.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ90lbl4kl6c",
        "outputId": "d76dac8a-18c4-4293-e312-6e1fd58be3a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to create topic 'SDMD-1072573_78_vehicle_positions': [Error 36] TopicAlreadyExistsError: Request 'CreateTopicsRequest_v3(create_topic_requests=[(topic='SDMD-1072573_78_vehicle_positions', num_partitions=3, replication_factor=1, replica_assignment=[], configs=[])], timeout=30000, validate_only=False)' failed with response 'CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='SDMD-1072573_78_vehicle_positions', error_code=36, error_message=\"Topic 'SDMD-1072573_78_vehicle_positions' already exists.\")])'.\n",
            "Sent data: {'vehicle_id': 'vehicle_1', 'latitude': -29.81684439745748, 'longitude': 124.37863711089778, 'timestamp': '2024-05-19T13:32:59.966020'}\n",
            "Sent data: {'vehicle_id': 'vehicle_2', 'latitude': -78.40161079875914, 'longitude': 129.38311370748994, 'timestamp': '2024-05-19T13:33:00.123085'}\n",
            "Sent data: {'vehicle_id': 'vehicle_3', 'latitude': 39.5188109920218, 'longitude': -61.544807982708605, 'timestamp': '2024-05-19T13:33:00.124029'}\n",
            "Stopping data transmission.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from kafka import KafkaProducer\n",
        "from kafka.admin import KafkaAdminClient, NewTopic\n",
        "import random\n",
        "\n",
        "def generate_vehicle_data(vehicle_id):\n",
        "    return {\n",
        "        \"vehicle_id\": vehicle_id,\n",
        "        \"latitude\": random.uniform(-90, 90),\n",
        "        \"longitude\": random.uniform(-180, 180),\n",
        "        \"timestamp\": datetime.utcnow().isoformat()\n",
        "    }\n",
        "\n",
        "def send_vehicle_data(bootstrap_servers, topic_name, interval, vehicle_ids):\n",
        "    producer = KafkaProducer(\n",
        "        bootstrap_servers=bootstrap_servers,\n",
        "        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            for vehicle_id in vehicle_ids:\n",
        "                data = generate_vehicle_data(vehicle_id)\n",
        "                producer.send(topic_name, value=data)\n",
        "                print(f\"Sent data: {data}\")\n",
        "            time.sleep(interval)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Stopping data transmission.\")\n",
        "    finally:\n",
        "        producer.close()\n",
        "\n",
        "topic_name = \"SDMD-1072573_78_vehicle_positions\"\n",
        "num_partitions = 3\n",
        "replication_factor = 1\n",
        "# Vehicle IDs to simulate\n",
        "vehicle_ids = [\"vehicle_1\", \"vehicle_2\", \"vehicle_3\"]\n",
        "interval = 5\n",
        "try: # ?\n",
        "  create_kafka_topic(kafka_broker, topic_name, num_partitions, replication_factor)\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "send_vehicle_data(kafka_broker, topic_name, interval, vehicle_ids)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rIjHPoZBo-b"
      },
      "source": [
        "#### Ερώτημα 1.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfMU_TuiBoxF"
      },
      "outputs": [],
      "source": [
        "from kafka import KafkaConsumer\n",
        "import json\n",
        "\n",
        "def consume_vehicle_data(bootstrap_servers, topic_name):\n",
        "    consumer = KafkaConsumer(\n",
        "    topic_name,\n",
        "    bootstrap_servers=[bootstrap_servers],\n",
        "    #api_version=KAFKA_API_VERSION,\n",
        "    auto_offset_reset=\"earliest\",\n",
        "    enable_auto_commit=True,)\n",
        "    try:\n",
        "      for message in consumer:\n",
        "          print(f\"Received message: {message.value}\")\n",
        "    except KeyboardInterrupt as e:\n",
        "        print(\"Stopping consumer.\")\n",
        "    finally:\n",
        "        consumer.close()\n",
        "\n",
        "consume_vehicle_data(kafka_broker, topic_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl9dyoT0EBpn"
      },
      "source": [
        "## Ερώτημα 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKCd0rLKFt2m"
      },
      "source": [
        "### Ερώτημα 2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "nlPX-s5lFMDA",
        "outputId": "8bacc6c3-feb0-4244-bbc5-6069a7b82eae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0️⃣   Install Java if not available\n",
            "✅ Java is already installed.\n",
            "\n",
            "1️⃣   Download and install Hadoop and Spark\n",
            "✅ https://dlcdn.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz was found\n",
            "✅ 2024-05-19 13:40:29 (57.5 MB/s) - ‘spark-3.5.1-bin-hadoop3.tgz’ saved [400446614/400446614]\n",
            "✅ Uncompressed Spark distribution\n",
            "\n",
            "\n",
            "2️⃣   Start Spark engine\n",
            "✅ no org.apache.spark.deploy.master.Master to stop\n",
            "✅ starting org.apache.spark.deploy.master.Master, logging to /content/spark-3.5.1-bin-hadoop3/logs/spark--org.apache.spark.deploy.master.Master-1-3ee69abf12bf.out\n",
            "✅ no org.apache.spark.deploy.worker.Worker to stop\n",
            "✅ starting org.apache.spark.deploy.worker.Worker, logging to /content/spark-3.5.1-bin-hadoop3/logs/spark--org.apache.spark.deploy.worker.Worker-1-3ee69abf12bf.out\n",
            "\n",
            "3️⃣   Start Master Web UI\n",
            "Search for port number in log file /content/spark-3.5.1-bin-hadoop3/logs/spark--org.apache.spark.deploy.master.Master-1-3ee69abf12bf.out\n",
            "✅ Master UI is available at localhost:8081 (attempt nr. 5)\n",
            "Click on the link below to open the Spark Web UI 🚀\n"
          ]
        },
        {
          "data": {
            "application/javascript": "(async (port, path, text, element) => {\n    if (!google.colab.kernel.accessAllowed) {\n      return;\n    }\n    element.appendChild(document.createTextNode(''));\n    const url = await google.colab.kernel.proxyPort(port);\n    const anchor = document.createElement('a');\n    anchor.href = new URL(path, url).toString();\n    anchor.target = '_blank';\n    anchor.setAttribute('data-href', url + path);\n    anchor.textContent = text;\n    element.appendChild(anchor);\n  })(8081, \"/\", \"https://localhost:8081/\", window.element)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "4️⃣   Start history server\n",
            "✅ no org.apache.spark.deploy.history.HistoryServer to stop\n",
            "✅ starting org.apache.spark.deploy.history.HistoryServer, logging to /content/spark-3.5.1-bin-hadoop3/logs/spark--org.apache.spark.deploy.history.HistoryServer-1-3ee69abf12bf.out\n",
            "Click on the link below to open the Spark History Server Web UI 🚀\n"
          ]
        },
        {
          "data": {
            "application/javascript": "(async (port, path, text, element) => {\n    if (!google.colab.kernel.accessAllowed) {\n      return;\n    }\n    element.appendChild(document.createTextNode(''));\n    const url = await google.colab.kernel.proxyPort(port);\n    const anchor = document.createElement('a');\n    anchor.href = new URL(path, url).toString();\n    anchor.target = '_blank';\n    anchor.setAttribute('data-href', url + path);\n    anchor.textContent = text;\n    element.appendChild(anchor);\n  })(18080, \"/\", \"https://localhost:18080/\", window.element)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import requests\n",
        "import subprocess\n",
        "import os\n",
        "import re\n",
        "import socket\n",
        "import shutil\n",
        "import time\n",
        "import sys\n",
        "\n",
        "def run(cmd):\n",
        "    # run a shell command\n",
        "    try:\n",
        "        # Run the command and capture stdout and stderr\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        # Access stdout (stderr redirected to stdout)\n",
        "        stdout_result = subprocess_output.stdout.strip().splitlines()[-1]\n",
        "        # Process the results as needed\n",
        "        print(f'✅ {stdout_result}')\n",
        "        return stdout_result\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(f\"Command failed with return code {e.returncode}\")\n",
        "        print(\"stdout:\", e.stdout)\n",
        "\n",
        "def is_java_installed():\n",
        "    return shutil.which(\"java\")\n",
        "\n",
        "def install_java():\n",
        "    # Uncomment and modify the desired version\n",
        "    # java_version= 'openjdk-11-jre-headless'\n",
        "    # java_version= 'default-jre'\n",
        "    # java_version= 'openjdk-17-jre-headless'\n",
        "    # java_version= 'openjdk-18-jre-headless'\n",
        "    java_version= 'openjdk-19-jre-headless'\n",
        "    os.environ['JAVA_HOME'] = ' /usr/lib/jvm/java-19-openjdk-amd64'\n",
        "    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n",
        "    try:\n",
        "        cmd = f\"apt install -y {java_version}\"\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        stdout_result = subprocess_output.stdout\n",
        "        # Process the results as needed\n",
        "        print(f'✅ Done installing Java {java_version}')\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(f\"Command failed with return code {e.returncode}\")\n",
        "        print(\"stdout:\", e.stdout)\n",
        "\n",
        "print(\"\\n0️⃣   Install Java if not available\")\n",
        "if is_java_installed():\n",
        "    print(\"✅ Java is already installed.\")\n",
        "else:\n",
        "    install_java()\n",
        "\n",
        "print(\"\\n1️⃣   Download and install Hadoop and Spark\")\n",
        "# URL for downloading Hadoop and Spark\n",
        "SPARK_VERSION = \"3.5.1\"\n",
        "HADOOP_SPARK_URL = \"https://dlcdn.apache.org/spark/spark-\" + SPARK_VERSION + \\\n",
        "                   \"/spark-\" + SPARK_VERSION + \"-bin-hadoop3.tgz\"\n",
        "r = requests.head(HADOOP_SPARK_URL)\n",
        "if r.status_code >= 200 and r.status_code < 400:\n",
        "    print(f'✅ {HADOOP_SPARK_URL} was found')\n",
        "else:\n",
        "    SPARK_CDN = \"https://dlcdn.apache.org/spark/\"\n",
        "    print(f'⚠️ {HADOOP_SPARK_URL} was NOT found. \\nCheck for available Spark versions in {SPARK_CDN}')\n",
        "\n",
        "# set some environment variables\n",
        "os.environ['SPARK_HOME'] = os.path.join(os.getcwd(), os.path.splitext(os.path.basename(HADOOP_SPARK_URL))[0])\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['SPARK_HOME'], 'bin'), os.environ['PATH']])\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['SPARK_HOME'], 'sbin'), os.environ['PATH']])\n",
        "\n",
        "# download Spark\n",
        "# using --no-clobber option will prevent wget from downloading file if already present\n",
        "# shell command: wget --no-clobber $HADOOP_SPARK_URL\n",
        "cmd = f\"wget --no-clobber {HADOOP_SPARK_URL}\"\n",
        "run(cmd)\n",
        "\n",
        "# uncompress\n",
        "try:\n",
        "    # Run the command and capture stdout and stderr\n",
        "    cmd = \"([ -d $(basename {0}|sed 's/\\.[^.]*$//') ] && echo -n 'Folder already exists') || (tar xzf $(basename {0}) && echo 'Uncompressed Spark distribution')\"\n",
        "    subprocess_output = subprocess.run(cmd.format(HADOOP_SPARK_URL), shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    # Access stdout (stderr redirected to stdout)\n",
        "    stdout_result = subprocess_output.stdout\n",
        "    # Process the results as needed\n",
        "    print(f'✅ {stdout_result}')\n",
        "\n",
        "except subprocess.CalledProcessError as e:\n",
        "    # Handle the error if the command returns a non-zero exit code\n",
        "    print(f\"Command failed with return code {e.returncode}\")\n",
        "    print(\"stdout:\", e.stdout)\n",
        "\n",
        "\n",
        "print(\"\\n2️⃣   Start Spark engine\")\n",
        "# start master\n",
        "# shell command: $SPARK_HOME/sbin/start-master.sh\n",
        "cmd = os.path.join(os.environ['SPARK_HOME'], 'sbin', 'stop-master.sh')\n",
        "run(cmd)\n",
        "cmd = os.path.join(os.environ['SPARK_HOME'], 'sbin', 'start-master.sh')\n",
        "out = run(cmd)\n",
        "\n",
        "# start one worker (first stop it in case it's already running)\n",
        "# shell command: $SPARK_HOME/sbin/start-worker.sh spark://${HOSTNAME}:7077\n",
        "cmd = [os.path.join(os.environ['SPARK_HOME'], 'sbin', 'stop-worker.sh')]\n",
        "run(cmd)\n",
        "cmd = os.path.join(os.environ['SPARK_HOME'], 'sbin', 'start-worker.sh') + ' ' + 'spark://'+socket.gethostname()+':7077'\n",
        "run(cmd)\n",
        "\n",
        "print(\"\\n3️⃣   Start Master Web UI\")\n",
        "# get master UI's port number\n",
        "# the subprocess that's starting the master with start-master.sh\n",
        "# might still not be ready with assigning the port number at this point\n",
        "# therefore we check the logfile a few times (attempts=5) to see if the port\n",
        "# has been assigned. This might take 1-2 seconds.\n",
        "\n",
        "master_log = out.partition(\"logging to\")[2].strip()\n",
        "print(\"Search for port number in log file {}\".format(master_log))\n",
        "attempts = 10\n",
        "search_pattern = \"Successfully started service 'MasterUI' on port (\\d+)\"\n",
        "found = False\n",
        "for i in range(attempts):\n",
        "  if not found:\n",
        "   with open(master_log) as log:\n",
        "      found = re.search(search_pattern, log.read())\n",
        "      if found:\n",
        "          webUIport = found.group(1)\n",
        "          print(f\"✅ Master UI is available at localhost:{webUIport} (attempt nr. {i})\")\n",
        "          break\n",
        "      else:\n",
        "          time.sleep(2) # need to try until port information is found in the logfile\n",
        "          i+=1\n",
        "if not found:\n",
        "  print(\"Could not find port for Master Web UI\\n\")\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    # serve the Web UI on Colab\n",
        "    print(\"Click on the link below to open the Spark Web UI 🚀\")\n",
        "    from google.colab import output\n",
        "    output.serve_kernel_port_as_window(webUIport)\n",
        "\n",
        "print(\"\\n4️⃣   Start history server\")\n",
        "# start history server\n",
        "# shell command: mkdir -p /tmp/spark-events\n",
        "# shell command: $SPARK_HOME/sbin/start-history-server.sh\n",
        "spark_events_dir = os.path.join('/tmp', 'spark-events')\n",
        "if not os.path.exists(spark_events_dir):\n",
        "    os.mkdir(spark_events_dir)\n",
        "cmd = os.path.join(os.environ['SPARK_HOME'], 'sbin', 'stop-history-server.sh')\n",
        "run(cmd)\n",
        "cmd = os.path.join(os.environ['SPARK_HOME'], 'sbin', 'start-history-server.sh')\n",
        "run(cmd)\n",
        "\n",
        "if IN_COLAB:\n",
        "    # serve the History Server\n",
        "    print(\"Click on the link below to open the Spark History Server Web UI 🚀\")\n",
        "    output.serve_kernel_port_as_window(18080)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1Kdo1p_GFfu"
      },
      "source": [
        "#### Ερώτημα 2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "I5Yw2Y_BGJeg",
        "outputId": "24aed363-a579-4c1a-8436-8b417ed644df"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, from_json, to_timestamp, window\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType, SubscribeType\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SDMD-1072573_78\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "lines = spark\\\n",
        "    .readStream\\\n",
        "    .format(\"kafka\")\\\n",
        "    .option(\"kafka.bootstrap.servers\", kafka_broker)\\\n",
        "    .option(StructType, topic_name)\\\n",
        "    .load()\n",
        "\n",
        "lines = lines\\\n",
        "    .selectExpr(\"CAST(value AS STRING) as json\")\n",
        "\n",
        "query = lines\\\n",
        "    .writeStream\\\n",
        "    .outputMode(\"append\")\\\n",
        "    .format(\"console\")\\\n",
        "    .start()\n",
        "# Terminates the stream on abort\n",
        "query.awaitTermination()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
